{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## BERT fine tune NER"
   ],
   "metadata": {
    "id": "IC1q9mqoR-3j"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Bert** is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labeling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts."
   ],
   "metadata": {
    "id": "3cfrmUZ0RBX0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**conll2003:**\n",
    "The shared task of CoNLL-2003 concerns language-independent named entity recognition. We will concentrate on four types of named entities: persons, locations, organizations and names of miscellaneous entities that do not belong to the previous three groups.\n",
    "\n",
    "The CoNLL-2003 shared task data files contain four columns separated by a single space. Each word has been put on a separate line and there is an empty line after each sentence. The first item on each line is a word, the second a part-of-speech (POS) tag, the third a syntactic chunk tag and the fourth the named entity tag. The chunk tags and the named entity tags have the format I-TYPE which means that the word is inside a phrase of type TYPE. Only if two phrases of the same type immediately follow each other, the first word of the second phrase will have tag B-TYPE to show that it starts a new phrase."
   ],
   "metadata": {
    "id": "iek9-W05ULw5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ZVwkhyvQssx",
    "outputId": "b39194c6-e369-4760-8403-024e995c1be1",
    "ExecuteTime": {
     "end_time": "2024-02-26T12:13:31.111980Z",
     "start_time": "2024-02-26T12:11:26.133027700Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/huggingface/transformers\n",
    "!pip install -q datasets\n",
    "!pip install -q seqeval"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "• 0 means the word doesn't correspond to any entity. \n",
    "• B-PER/I-PER means the word corresponds to the beginning of/is inside a person entity. \n",
    "• B-ORG/I-ORG means the word corresponds to the beginning of/is inside an organization entity. \n",
    "• B-LOC/I-LOC means the word corresponds to the beginning of/is inside a location entity. \n",
    "• B-MISC/I-MISC means the word corresponds to the beginning of/is inside a miscellaneous entity. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModel, AutoTokenizer, DataCollator\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "id": "3ilEUMVjSeJ4",
    "ExecuteTime": {
     "end_time": "2024-02-27T11:26:44.277225900Z",
     "start_time": "2024-02-27T11:26:23.376548Z"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DeusExMachina\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"conll2003\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T11:26:50.153288200Z",
     "start_time": "2024-02-27T11:26:44.279226400Z"
    }
   },
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "Learning POS Tagging & Chunking in NLP: https://medium.com/greyatom/learning-pos-tagging-chunking-in-nlp-85f7f811a8cb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n        num_rows: 14041\n    })\n    validation: Dataset({\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n        num_rows: 3250\n    })\n    test: Dataset({\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n        num_rows: 3453\n    })\n})"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T11:26:50.163017200Z",
     "start_time": "2024-02-27T11:26:50.154289100Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'train': (14041, 5), 'validation': (3250, 5), 'test': (3453, 5)}"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T11:26:50.175531300Z",
     "start_time": "2024-02-27T11:26:50.163017200Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'id': Value(dtype='string', id=None),\n 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n 'pos_tags': Sequence(feature=ClassLabel(names=['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB'], id=None), length=-1, id=None),\n 'chunk_tags': Sequence(feature=ClassLabel(names=['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP'], id=None), length=-1, id=None),\n 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)}"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T11:26:50.206242800Z",
     "start_time": "2024-02-27T11:26:50.173526400Z"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0', 'tokens': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7], 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0], 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "\n",
    "print(dataset['train'][idx])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T11:26:50.208239600Z",
     "start_time": "2024-02-27T11:26:50.181988600Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].features['ner_tags']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T11:27:54.163664Z",
     "start_time": "2024-02-27T11:27:54.153419900Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'The shared task of CoNLL-2003 concerns language-independent named entity recognition. We will concentrate on\\nfour types of named entities: persons, locations, organizations and names of miscellaneous entities that do\\nnot belong to the previous three groups.\\n\\nThe CoNLL-2003 shared task data files contain four columns separated by a single space. Each word has been put on\\na separate line and there is an empty line after each sentence. The first item on each line is a word, the second\\na part-of-speech (POS) tag, the third a syntactic chunk tag and the fourth the named entity tag. The chunk tags\\nand the named entity tags have the format I-TYPE which means that the word is inside a phrase of type TYPE. Only\\nif two phrases of the same type immediately follow each other, the first word of the second phrase will have tag\\nB-TYPE to show that it starts a new phrase. A word with tag O is not part of a phrase. Note the dataset uses IOB2\\ntagging scheme, whereas the original dataset uses IOB1.\\n\\nFor more details see https://www.clips.uantwerpen.be/conll2003/ner/ and https://www.aclweb.org/anthology/W03-0419\\n'"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].description"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T11:29:59.282687800Z",
     "start_time": "2024-02-27T11:29:59.276266900Z"
    }
   },
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "checkpoint = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T11:31:22.517056400Z",
     "start_time": "2024-02-27T11:31:21.943569100Z"
    }
   },
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0', 'tokens': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7], 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0], 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "sample = dataset['train'][0]\n",
    "print(sample)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T11:33:39.666155200Z",
     "start_time": "2024-02-27T11:33:39.655916300Z"
    }
   },
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "**tokenizer(input):** When you use tokenizer(input), you're essentially tokenizing the input text. This means breaking down the input text into individual tokens, which could be words, subwords, or characters, depending on the tokenizer used. This operation returns a list of tokens.\n",
    "tokenizer.encode(input):\n",
    "\n",
    "**tokenizer.encode(input):** goes one step further than just tokenizing. It not only tokenizes the input but also converts those tokens into corresponding numerical IDs, often referred to as token IDs or input IDs. These numerical IDs are what the model actually operates on. This operation returns a list of token IDs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 7327, 19164, 2446, 2655, 2000, 17757, 2329, 12559, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Returns only input_ids but tokenizer(input) returns: input_ids, token_type_ids, attention_mask\n",
    "\"\"\"\n",
    "print(tokenizer.encode(' '.join(sample['tokens']), add_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T11:45:39.345426400Z",
     "start_time": "2024-02-27T11:45:39.336894Z"
    }
   },
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenized_sample = tokenizer(sample['tokens'], is_split_into_words=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T11:40:14.288381700Z",
     "start_time": "2024-02-27T11:40:14.274117800Z"
    }
   },
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': [101, 7327, 19164, 2446, 2655, 2000, 17757, 2329, 12559, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sample"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T11:40:17.814524500Z",
     "start_time": "2024-02-27T11:40:17.802820800Z"
    }
   },
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_sample['input_ids'])\n",
    "print(tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T11:47:00.444003800Z",
     "start_time": "2024-02-27T11:47:00.433485500Z"
    }
   },
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, None]"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids = tokenized_sample.word_ids() # return list of mapping the tokens, to their actual word in the initial sentence\n",
    "word_ids"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T11:47:19.607156700Z",
     "start_time": "2024-02-27T11:47:19.599256200Z"
    }
   },
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(9, 11)"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample['ner_tags']), len(tokenized_sample['input_ids'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T11:49:06.478463Z",
     "start_time": "2024-02-27T11:49:06.462139100Z"
    }
   },
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "source": [
    "Size between the ner_tags in the sample text and the tokenized input_ids from the sample (One is larger)\n",
    "-> **PROBLEM OF SUB-TOKEN**\n",
    "WHY? because transformers are often trainer on sub_words tokenizers, so even if we give splitted inputs, it can be splitted again by the tokenizer. And also because some special tokens maybe added like ('[CLS]', '[SEP]')\n",
    "-> **Solution**\n",
    "Get based on `.word_ids()` method because it sets the special tokens to `None`. We will set the labels for all special tokens to $-100$ because it gets ignored by pytorch during training. And all other tokens to the word they come from.  "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(example, label_all_tokens = True):\n",
    "    \"\"\"\n",
    "    This function will do two things:\n",
    "        1. Set -100 as the label for special tokens\n",
    "        2. Mask the sub-words representation after the first sub-word\n",
    "    Parameters:\n",
    "        example (str): the dataset\n",
    "        label_all_tokens (bool): define if we will apply tokenization\n",
    "    return: \n",
    "    \"\"\"\n",
    "    \n",
    "    tokenized_example = tokenizer(example['tokens'], is_split_into_words=True, truncation=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(example['ner_tags']):\n",
    "        word_ids = tokenized_example.word_ids(batch_index=i)\n",
    "        prev_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != prev_word_idx:\n",
    "                # common scenario\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                # take care of sub-words\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            prev_word_idx = word_idx    \n",
    "        labels.append(label_ids)\n",
    "    tokenized_example['labels'] = labels\n",
    "    return tokenized_example\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T12:20:41.514470900Z",
     "start_time": "2024-02-27T12:20:41.505343100Z"
    }
   },
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 2002, 2056, 2582, 4045, 2817, 2001, 3223, 1998, 2065, 2009, 2001, 2179, 2008, 2895, 2001, 2734, 2009, 2323, 2022, 2579, 2011, 1996, 2647, 2586, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, -100]]}\n"
     ]
    }
   ],
   "source": [
    "idx = 6\n",
    "print(tokenize_and_align_labels(dataset['train'][idx:idx+1]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T12:31:47.542817700Z",
     "start_time": "2024-02-27T12:31:47.534666300Z"
    }
   },
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "source": [
    "`labels` is added, represente the aligned values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
