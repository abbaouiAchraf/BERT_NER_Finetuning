{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IC1q9mqoR-3j"
   },
   "source": [
    "## BERT fine tune NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cfrmUZ0RBX0"
   },
   "source": [
    "**Bert** is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labeling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iek9-W05ULw5"
   },
   "source": [
    "**conll2003:**\n",
    "The shared task of CoNLL-2003 concerns language-independent named entity recognition. We will concentrate on four types of named entities: persons, locations, organizations and names of miscellaneous entities that do not belong to the previous three groups.\n",
    "\n",
    "The CoNLL-2003 shared task data files contain four columns separated by a single space. Each word has been put on a separate line and there is an empty line after each sentence. The first item on each line is a word, the second a part-of-speech (POS) tag, the third a syntactic chunk tag and the fourth the named entity tag. The chunk tags and the named entity tags have the format I-TYPE which means that the word is inside a phrase of type TYPE. Only if two phrases of the same type immediately follow each other, the first word of the second phrase will have tag B-TYPE to show that it starts a new phrase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- Part of Speech (POS) Tagging:\n",
    "\n",
    "POS tagging involves labeling each word in a sentence with its corresponding part of speech, such as noun, verb, adjective, adverb, etc.\n",
    "The primary goal of POS tagging is to assign grammatical categories to words in a sentence, which helps in understanding the syntactic structure of the sentence and its meaning.\n",
    "POS tagging provides information about the grammatical roles of individual words in a sentence.\n",
    "\n",
    "- Chunking:\n",
    "\n",
    "Chunking, also known as shallow parsing, involves identifying and extracting meaningful phrases or \"chunks\" from sentences based on their grammatical structure.\n",
    "Unlike POS tagging, which assigns part of speech tags to individual words, chunking groups words together into syntactically meaningful units, such as noun phrases, verb phrases, prepositional phrases, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "• 0 means the word doesn't correspond to any entity. \n",
    "• B-PER/I-PER means the word corresponds to the beginning of/is inside a person entity. \n",
    "• B-ORG/I-ORG means the word corresponds to the beginning of/is inside an organization entity. \n",
    "• B-LOC/I-LOC means the word corresponds to the beginning of/is inside a location entity. \n",
    "• B-MISC/I-MISC means the word corresponds to the beginning of/is inside a miscellaneous entity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T11:10:55.318765800Z",
     "start_time": "2024-02-28T11:10:55.307448Z"
    },
    "id": "3ilEUMVjSeJ4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModel, AutoTokenizer, DataCollator\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T08:45:52.098359400Z",
     "start_time": "2024-02-28T08:45:45.902201900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T08:45:52.119993400Z",
     "start_time": "2024-02-28T08:45:52.100360200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T08:45:52.121570400Z",
     "start_time": "2024-02-28T08:45:52.105978Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (14041, 5), 'validation': (3250, 5), 'test': (3453, 5)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T08:45:52.168085400Z",
     "start_time": "2024-02-28T08:45:52.116993600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='string', id=None),\n",
       " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'pos_tags': Sequence(feature=ClassLabel(names=['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB'], id=None), length=-1, id=None),\n",
       " 'chunk_tags': Sequence(feature=ClassLabel(names=['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP'], id=None), length=-1, id=None),\n",
       " 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T11:26:50.208239600Z",
     "start_time": "2024-02-27T11:26:50.181988600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0', 'tokens': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7], 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0], 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "\n",
    "print(dataset['train'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T11:27:54.163664Z",
     "start_time": "2024-02-27T11:27:54.153419900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].features['ner_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T08:48:53.559664600Z",
     "start_time": "2024-02-28T08:48:53.544776100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The shared task of CoNLL-2003 concerns language-independent named entity recognition. We will concentrate on\\nfour types of named entities: persons, locations, organizations and names of miscellaneous entities that do\\nnot belong to the previous three groups.\\n\\nThe CoNLL-2003 shared task data files contain four columns separated by a single space. Each word has been put on\\na separate line and there is an empty line after each sentence. The first item on each line is a word, the second\\na part-of-speech (POS) tag, the third a syntactic chunk tag and the fourth the named entity tag. The chunk tags\\nand the named entity tags have the format I-TYPE which means that the word is inside a phrase of type TYPE. Only\\nif two phrases of the same type immediately follow each other, the first word of the second phrase will have tag\\nB-TYPE to show that it starts a new phrase. A word with tag O is not part of a phrase. Note the dataset uses IOB2\\ntagging scheme, whereas the original dataset uses IOB1.\\n\\nFor more details see https://www.clips.uantwerpen.be/conll2003/ner/ and https://www.aclweb.org/anthology/W03-0419\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T08:49:05.922578900Z",
     "start_time": "2024-02-28T08:49:05.910581Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ner_tags = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T08:50:39.567328700Z",
     "start_time": "2024-02-28T08:50:38.877005100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "checkpoint = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T08:50:40.979082800Z",
     "start_time": "2024-02-28T08:50:40.973085800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0', 'tokens': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7], 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0], 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "sample = dataset['train'][0]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**tokenizer(input):** When you use tokenizer(input), you're essentially tokenizing the input text. This means breaking down the input text into individual tokens, which could be words, subwords, or characters, depending on the tokenizer used. This operation returns a list of tokens.\n",
    "\n",
    "**tokenizer.encode(input):** goes one step further than just tokenizing. It not only tokenizes the input but also converts those tokens into corresponding numerical IDs, often referred to as token IDs or input IDs. These numerical IDs are what the model actually operates on. This operation returns a list of token IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T08:51:38.875443700Z",
     "start_time": "2024-02-28T08:51:38.839975500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 7327, 19164, 2446, 2655, 2000, 17757, 2329, 12559, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Returns only input_ids but tokenizer(input) returns: input_ids, token_type_ids, attention_mask\n",
    "\"\"\"\n",
    "print(tokenizer.encode(' '.join(sample['tokens']), add_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T08:51:49.872431900Z",
     "start_time": "2024-02-28T08:51:49.863424200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_sample = tokenizer(sample['tokens'], is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T08:51:50.879373200Z",
     "start_time": "2024-02-28T08:51:50.869024400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7327, 19164, 2446, 2655, 2000, 17757, 2329, 12559, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The output of the code is a dictionary with three keys:\n",
    "\n",
    "``input_ids``: This is a list of integers that represent the numerical representation of the input text. Each integer corresponds to a token in the vocabulary of the pre-trained model.\n",
    "``token_type_ids``: This is a list of integers that indicate the type of each token in the input sequence. For example, in a sequence classification task, the first token of the input sequence could be marked as type 0, and the second token as type 1.\n",
    "``attention_mask``: This is a list of 1's and 0's that indicate which tokens should be attended to by the pre-trained model and which should be ignored. A 1 indicates that the token should be attended to, while a 0 indicates that the token should be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T09:00:55.476893800Z",
     "start_time": "2024-02-28T09:00:55.466696900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_sample['input_ids'])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T09:01:07.017952700Z",
     "start_time": "2024-02-28T09:01:07.009556700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, None]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids = tokenized_sample.word_ids() # return list of mapping the tokens, to their actual word in the initial sentence\n",
    "word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T09:01:19.737580900Z",
     "start_time": "2024-02-28T09:01:19.724033800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 11)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample['ner_tags']), len(tokenized_sample['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Size between the ner_tags in the sample text and the tokenized input_ids from the sample (One is larger)\n",
    "-> **PROBLEM OF SUB-TOKEN**\n",
    "WHY? because transformers are often trainer on sub_words tokenizers, so even if we give splitted inputs, it can be splitted again by the tokenizer. And also because some special tokens maybe added like ('[CLS]', '[SEP]')\n",
    "-> **Solution**\n",
    "Get based on `.word_ids()` method because it sets the special tokens to `None`. We will set the labels for all special tokens to $-100$ because it gets ignored by pytorch during training. And all other tokens to the word they come from.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T15:06:24.986904100Z",
     "start_time": "2024-02-28T15:06:24.979400500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(example, label_all_tokens = True):\n",
    "    \"\"\"\n",
    "    This function will do two things:\n",
    "        1. Set -100 as the label for special tokens\n",
    "        2. Mask the sub-words representation after the first sub-word\n",
    "    Parameters:\n",
    "        example (str): the dataset\n",
    "        label_all_tokens (bool): define if we will apply tokenization\n",
    "    return: \n",
    "    \"\"\"\n",
    "    \n",
    "    tokenized_example = tokenizer(example['tokens'], is_split_into_words=True, truncation=True, \n",
    "        padding=\"max_length\",  # Add padding\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(example['ner_tags']):\n",
    "        word_ids = tokenized_example.word_ids(batch_index=i)\n",
    "        prev_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != prev_word_idx:\n",
    "                # common scenario\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                # take care of sub-words\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            prev_word_idx = word_idx    \n",
    "        labels.append(label_ids)\n",
    "    tokenized_example['labels'] = labels\n",
    "    return tokenized_example\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T15:06:25.331478300Z",
     "start_time": "2024-02-28T15:06:25.319149400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 7327, 19164, 2446, 2655, 2000, 17757, 2329, 12559, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2848, 13934, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'labels': [[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [-100, 1, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]]}\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "benchmarking = tokenize_and_align_labels(dataset['train'][idx:idx+2])\n",
    "print(benchmarking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "`labels` is added, represente the aligned values\n",
    "Objectif was to have a new labels that matches the tokenized input_ids size from ner_tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T15:06:26.500216400Z",
     "start_time": "2024-02-28T15:06:26.488494900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101___________________________ -100\n",
      "7327__________________________ 3\n",
      "19164_________________________ 0\n",
      "2446__________________________ 7\n",
      "2655__________________________ 0\n",
      "2000__________________________ 0\n",
      "17757_________________________ 0\n",
      "2329__________________________ 7\n",
      "12559_________________________ 0\n",
      "1012__________________________ 0\n",
      "102___________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n",
      "0_____________________________ -100\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(benchmarking['input_ids'][0], benchmarking['labels'][0]):\n",
    "    print(f'{token:_<30} {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T15:06:37.136735500Z",
     "start_time": "2024-02-28T15:06:27.051144300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T15:06:37.155353200Z",
     "start_time": "2024-02-28T15:06:37.150533Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 14041\n",
       "})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T15:06:37.206791800Z",
     "start_time": "2024-02-28T15:06:37.157744300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ner_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        return {\n",
    "            'input_ids': torch.LongTensor(sample['input_ids']),\n",
    "            'token_type_ids': torch.LongTensor(sample['token_type_ids']),\n",
    "            'attention_mask': torch.LongTensor(sample['attention_mask']),\n",
    "            'labels': torch.LongTensor(sample['labels'])\n",
    "        }    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T15:06:37.278825700Z",
     "start_time": "2024-02-28T15:06:37.200792Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BATCH = 8\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "train_data = ner_dataset(tokenized_dataset['train'])\n",
    "validation_data = ner_dataset(tokenized_dataset['validation'])\n",
    "test_data = ner_dataset(tokenized_dataset['test'])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH, shuffle=True)\n",
    "validation_loader = DataLoader(validation_data, batch_size=BATCH, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T15:06:37.370049Z",
     "start_time": "2024-02-28T15:06:37.280826400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101, 12809,  5158,  ...,     0,     0,     0],\n",
      "        [  101,  2009,  2097,  ...,     0,     0,     0],\n",
      "        [  101, 13893,  2920,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101, 28669,  2310,  ...,     0,     0,     0],\n",
      "        [  101,  2008,  2933,  ...,     0,     0,     0],\n",
      "        [  101,  2035,  2008,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[-100,    0,    0,  ..., -100, -100, -100],\n",
      "        [-100,    0,    0,  ..., -100, -100, -100],\n",
      "        [-100,    0,    0,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [-100,    1,    2,  ..., -100, -100, -100],\n",
      "        [-100,    0,    0,  ..., -100, -100, -100],\n",
      "        [-100,    0,    0,  ..., -100, -100, -100]])}\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T09:53:19.574767900Z",
     "start_time": "2024-02-28T09:53:18.806835800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.9127, -0.0694,  0.0756,  ..., -0.5373,  0.2770,  0.0182],\n",
      "         [ 0.3057, -0.3440, -0.1638,  ..., -0.4052,  0.9113,  0.1845],\n",
      "         [-0.6852, -0.1045,  0.0471,  ..., -0.6598, -0.4638,  0.1579],\n",
      "         ...,\n",
      "         [ 1.1856,  0.3611,  0.0776,  ..., -0.1451,  0.0478, -0.0247],\n",
      "         [-0.6083, -0.7390, -0.3228,  ...,  0.0873, -0.0148, -0.1238],\n",
      "         [ 0.6173,  0.1620, -0.4447,  ...,  0.0409, -0.4653, -0.1896]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor(benchmarking['input_ids'])\n",
    "attention_mask = torch.tensor(benchmarking['attention_mask'])\n",
    "token_type_ids = torch.tensor(benchmarking['token_type_ids'])\n",
    "\n",
    "# Load the model\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T09:55:37.749484400Z",
     "start_time": "2024-02-28T09:55:37.726723200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BertForNER(torch.nn.Module):\n",
    "    def __init__(self, checkpoint, n_classes):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(checkpoint)\n",
    "        self.classifier = torch.nn.Linear(self.model.config.hidden_size, n_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask = None, token_type_ids = None):\n",
    "        output = self.model(input_ids, attention_mask, token_type_ids)\n",
    "        result = output[0]\n",
    "        logits = self.classifier(result)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T09:59:07.725020500Z",
     "start_time": "2024-02-28T09:59:06.497674400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = BertForNER(checkpoint,n_classes = 9).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T15:11:13.100107400Z",
     "start_time": "2024-02-28T15:11:13.074233Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-28T15:11:47.605233800Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to('cuda')\n",
    "        attention_mask = batch['attention_mask'].to('cuda')\n",
    "        token_type_ids = batch['token_type_ids'].to('cuda')\n",
    "        labels = batch['labels'].to('cuda')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_ids, attention_mask, token_type_ids)\n",
    "        output = output.view(-1, output.shape[-1])\n",
    "        labels = labels.view(-1)\n",
    "        loss_value = loss(output, labels)\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch: {epoch + 1}/{epoch}, Loss: {loss_value.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "for batch in validation_loader:\n",
    "    input_ids = batch['input_ids'].to('cuda')\n",
    "    attention_mask = batch['attention_mask'].to('cuda')\n",
    "    token_type_ids = batch['token_type_ids'].to('cuda')\n",
    "    labels = batch['labels'].to('cuda')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, attention_mask, token_type_ids)\n",
    "        output = output.view(-1, output.shape[-1])\n",
    "        labels = labels.view(-1)\n",
    "        loss_value = loss(output, labels)\n",
    "    print(f'Validation Loss: {loss_value.item()}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
